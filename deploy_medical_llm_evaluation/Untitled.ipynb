{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ced5b7d2-3a53-4748-bed6-0febcd3478b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'owlready2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Additional imports for synonyms and lemmatization\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mowlready2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mowlready2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpymedtermino2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mowlready2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpymedtermino2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mumls\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'owlready2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import spacy\n",
    "import json\n",
    "\n",
    "# Additional imports for synonyms and lemmatization\n",
    "from owlready2 import *\n",
    "from owlready2.pymedtermino2 import *\n",
    "from owlready2.pymedtermino2.umls import *\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcff95f5-97ae-4607-bbb3-3d895bf0f98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import spacy\n",
    "import json\n",
    "\n",
    "# Additional imports for synonyms and lemmatization\n",
    "from pymedtermino2 import get_ontology  # Ensure PyMedTermino2 is installed and configured\n",
    "import stanza\n",
    "\n",
    "# Load stanza pipeline once for efficiency\n",
    "stanza.download('en', package='mimic', processors='tokenize,pos,lemma')\n",
    "biomed_nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma', package='mimic')\n",
    "\n",
    "# Define the cleaning function\n",
    "def clean_text(text):\n",
    "    # Replace newline characters with a space and remove asterisks\n",
    "    text = text.replace('\\n', ' ').replace('*', '')\n",
    "    # Remove any extra spaces that may have resulted\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# Function to fetch synonyms\n",
    "def get_synonyms(entity):\n",
    "    \"\"\"\n",
    "    Retrieve synonyms for a given entity using PyMedTermino2.\n",
    "    Args:\n",
    "        entity (str): The clinical entity for which synonyms are needed.\n",
    "    Returns:\n",
    "        list: A list of synonyms for the entity.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        PYM = get_ontology(\"http://PYM/\").load()\n",
    "        SNOMEDCT_US = PYM[\"SNOMEDCT_US\"]\n",
    "        concept = SNOMEDCT_US.search(entity)  # Search for the entity in SNOMED CT\n",
    "        if concept:\n",
    "            concept = concept[0]  # Take the first match\n",
    "            synonyms = [str(term) for term in concept.label]\n",
    "            return synonyms\n",
    "        else:\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving synonyms for {entity}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to compute f1 with synonyms\n",
    "def compute_f1_with_synonyms(entities_true, entities_answer):\n",
    "    \"\"\"\n",
    "    Compute precision, recall, and F1 score using synonym-based intersection.\n",
    "    \n",
    "    Args:\n",
    "        entities_true (set): Set of entities from the true sentence.\n",
    "        entities_answer (set): Set of entities from the predicted sentence.\n",
    "        \n",
    "    Returns:\n",
    "        precision, recall, f1: Computed scores.\n",
    "    \"\"\"\n",
    "    def expand_with_synonyms(entities):\n",
    "        expanded = {}\n",
    "        for entity in entities:\n",
    "            synonyms = get_synonyms(entity)  # Retrieve synonyms using PyMedTermino2\n",
    "            # Include the original entity as well, for direct matches\n",
    "            expanded[entity] = set(synonyms + [entity])\n",
    "        return expanded\n",
    "\n",
    "    # Expand entities with synonyms\n",
    "    true_synonyms = expand_with_synonyms(entities_true)\n",
    "    answer_synonyms = expand_with_synonyms(entities_answer)\n",
    "    \n",
    "    # Compute intersection based on synonym matches\n",
    "    intersection_count = 0\n",
    "    used_answer_entities = set()\n",
    "    for true_entity, true_synonyms_set in true_synonyms.items():\n",
    "        matched = False\n",
    "        for answer_entity, answer_synonyms_set in answer_synonyms.items():\n",
    "            if answer_entity not in used_answer_entities:\n",
    "                if not true_synonyms_set.isdisjoint(answer_synonyms_set):\n",
    "                    intersection_count += 1\n",
    "                    used_answer_entities.add(answer_entity)\n",
    "                    matched = True\n",
    "                    break\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = intersection_count / len(entities_answer) if len(entities_answer) > 0 else 0.0\n",
    "    recall = intersection_count / len(entities_true) if len(entities_true) > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Lemmatization function\n",
    "def lemmatize_entities(entities):\n",
    "    \"\"\"\n",
    "    Lemmatize a set of entity strings using Stanza's en_biomedical pipeline.\n",
    "    \n",
    "    Args:\n",
    "        entities (set): A set of entity strings to be lemmatized.\n",
    "    Returns:\n",
    "        set: A set of lemmatized entity strings.\n",
    "    \"\"\"\n",
    "    lemmatized_entities = set()\n",
    "    for entity in entities:\n",
    "        doc = biomed_nlp(entity)\n",
    "        lemmatized_entity = \" \".join([word.lemma for sentence in doc.sentences for word in sentence.words])\n",
    "        lemmatized_entities.add(lemmatized_entity)\n",
    "    return lemmatized_entities\n",
    "\n",
    "def parsing_and_computing_f1(input_answer_dir, model_list, rephrased=False):\n",
    "    # List of scispaCy models to use\n",
    "    nlp_models = {\n",
    "        'en_core_sci_lg': spacy.load('en_core_sci_lg'),\n",
    "        # 'en_core_sci_scibert': spacy.load('en_core_sci_scibert')\n",
    "    }\n",
    "    \n",
    "    # Initialize data collection\n",
    "    data_records = []\n",
    "    category_ids = [str(num) for num in range(1, 7)]  \n",
    "    iteration_numbers = [1, 2, 3]\n",
    "    \n",
    "    # Process each file with each scispaCy model\n",
    "    for nlp_name, nlp in nlp_models.items():\n",
    "        print(f\"Processing with model: {nlp_name}\")\n",
    "        for model in model_list:\n",
    "            for category_id in category_ids:\n",
    "                for iteration_number in iteration_numbers:\n",
    "                    # Construct file names\n",
    "                    answer_file_name = f\"{model}_answers_category_{category_id}.{iteration_number}_HIV_EQ.json\"\n",
    "                    input_answer_model = os.path.join(input_answer_dir, f\"raw/{model}/\")\n",
    "                    file_path = os.path.join(input_answer_model, answer_file_name)\n",
    "                    \n",
    "                    # Check if the file exists\n",
    "                    if not os.path.exists(file_path):\n",
    "                        print(f\"File not found: {file_path}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Load JSON data\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                    \n",
    "                    # Process each answer pair\n",
    "                    for idx, item in enumerate(data):\n",
    "                        true_answer = item.get('true_answer', '')\n",
    "                        answer = item.get('answer', '')\n",
    "                        question_index = idx\n",
    "    \n",
    "                        # Clean the 'true_answer' and 'answer' strings\n",
    "                        true_answer_clean = clean_text(true_answer)\n",
    "                        answer_clean = clean_text(answer)\n",
    "    \n",
    "                        # Extract entities using scispaCy\n",
    "                        doc_true = nlp(true_answer_clean)\n",
    "                        doc_answer = nlp(answer_clean)\n",
    "    \n",
    "                        # Proceed with entity extraction\n",
    "                        entities_true = set(ent.text.lower() for ent in doc_true.ents)\n",
    "                        entities_answer = set(ent.text.lower() for ent in doc_answer.ents)\n",
    "    \n",
    "                        # --- Original exact match F1 ---\n",
    "                        if entities_answer:\n",
    "                            intersection = entities_true & entities_answer\n",
    "                            precision = len(intersection) / len(entities_answer) if len(entities_answer) > 0 else 0.0\n",
    "                            recall = len(intersection) / len(entities_true) if len(entities_true) > 0 else 0.0\n",
    "                            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "                        else:\n",
    "                            precision = 0.0\n",
    "                            recall = 0.0\n",
    "                            f1 = 0.0\n",
    "    \n",
    "                        # --- F1 with synonyms ---\n",
    "                        synonyms_precision, synonyms_recall, synonyms_f1 = compute_f1_with_synonyms(entities_true, entities_answer)\n",
    "    \n",
    "                        # --- F1 with synonyms + lemmatization ---\n",
    "                        # First lemmatize the entities\n",
    "                        entities_true_lemmatized = lemmatize_entities(entities_true)\n",
    "                        entities_answer_lemmatized = lemmatize_entities(entities_answer)\n",
    "                        synonyms_lemma_precision, synonyms_lemma_recall, synonyms_lemma_f1 = compute_f1_with_synonyms(entities_true_lemmatized, entities_answer_lemmatized)\n",
    "    \n",
    "                        # Append results to data_records\n",
    "                        data_records.append({\n",
    "                            'nlp_model': nlp_name,\n",
    "                            'model': model,\n",
    "                            'category_id': category_id,\n",
    "                            'iteration_number': iteration_number,\n",
    "                            'question_index': question_index,\n",
    "                            'precision': precision,\n",
    "                            'recall': recall,\n",
    "                            'f1_score': f1,\n",
    "                            'synonyms_precision': synonyms_precision,\n",
    "                            'synonyms_recall': synonyms_recall,\n",
    "                            'synonyms_f1': synonyms_f1,\n",
    "                            'synonyms_lemmatized_precision': synonyms_lemma_precision,\n",
    "                            'synonyms_lemmatized_recall': synonyms_lemma_recall,\n",
    "                            'synonyms_lemmatized_f1': synonyms_lemma_f1\n",
    "                        })\n",
    "    \n",
    "    # Save data_records to a JSON file\n",
    "    os.makedirs('./evaluation_results', exist_ok=True)\n",
    "    with open('./evaluation_results/f1_results.json', 'w') as f:\n",
    "        json.dump(data_records, f, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    model_answers_files_path = os.path.join(script_dir, '/model_answers/')\n",
    "    model_list = [\"Claude\"]\n",
    "    \n",
    "    # Run the F1 score calculation\n",
    "    parsing_and_computing_f1(model_answers_files_path, model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e943072c-ebb2-40c7-b646-92a49dd987cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_clinical_text(text):\n",
    "    \"\"\"\n",
    "    Lemmatize clinical text using Stanza's en_biomedical pipeline.\n",
    "    Args:\n",
    "        text (str): Input clinical text to be lemmatized.\n",
    "    Returns:\n",
    "        str: Lemmatized text.\n",
    "    \"\"\"\n",
    "    import stanza\n",
    "\n",
    "    # Load the biomedical model\n",
    "    nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma', package='mimic')\n",
    "\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Reconstruct lemmatized text\n",
    "    lemmatized_text = \" \".join([word.lemma for sentence in doc.sentences for word in sentence.words])\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad6142f-2b8b-45f5-baf2-7269c5265817",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"Based on the patient's symptoms and laboratory results, the most likely diagnosis is Bacillary Angiomatosis (BA), a skin infection caused by Bartonella henselae or Bartonella quintana, commonly seen in immunocompromised patients, particularly those with HIV/AIDS. The presence of white patches on the palate that can be scraped off suggests oral thrush, a fungal infection, but the primary focus is on treating the skin lesions. The patient's low CD4+ T-lymphocyte count (98/mm3) indicates severe immunosuppression, and the biopsy results showing vascular proliferation and small black bacteria on Warthin-Starry stain confirm the diagnosis of BA. The most appropriate pharmacotherapy for Bacillary Angiomatosis in this patient is: 1. **Doxycycline** (100 mg orally twice a day) for 3-4 months, as it is the first-line treatment for BA. 2. **Erythromycin** (500 mg orally four times a day) can be used as an alternative, especially in patients with intolerance to doxycycline. It is also essential to consider antiretroviral therapy (ART) to manage the patient's HIV infection, but this is not directly related to the treatment of BA. In addition to pharmacotherapy, the patient's overall health, including her nutritional status, smoking, and substance use, should be addressed to prevent further complications and improve her quality of life.\"\n",
    "lemmatized_output = lemmatize_clinical_text(example_text)\n",
    "print(\"Original Text:\", example_text)\n",
    "print(\"Lemmatized Text:\", lemmatized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f151fd-4a5e-4434-93e5-5583eed93fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
